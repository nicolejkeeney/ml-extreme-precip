{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae7bca3-3901-4b51-8940-fb3f099c30e3",
   "metadata": {},
   "source": [
    "# Random Forest: Extreme Precipitation\n",
    "\n",
    "This notebook builds upon code from [rf_regress_christman.ipynb](https://github.com/eabarnes1010/course_ml_ats/blob/main/code/rf_regress_christman.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da132c14-785c-4355-9eb4-3c01914e8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.inspection import permutation_importance\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefd219-454e-4828-baf0-49b03b5c36cb",
   "metadata": {},
   "source": [
    "## Import input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dac76b-3b99-44f4-9f42-720ced37372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labels csv as pandas DataFrame object \n",
    "labels_df = pd.read_csv(\"../data/input_data_preprocessed/precip_classes.csv\")\n",
    "\n",
    "# Read features csv as pandas DataFrame object\n",
    "features_df = pd.read_csv(\"../data/input_data_preprocessed/slp_hgt_anoms.csv\")\n",
    "\n",
    "# Merge DataFrames on time column \n",
    "# Will ensure that the data has the same time index \n",
    "input_df = labels_df.merge(features_df, on=\"time\")\n",
    "\n",
    "# Format time \n",
    "datetime_np = pd.to_datetime(input_df[\"time\"].values)\n",
    "input_df[\"year\"] = datetime_np.year\n",
    "input_df[\"month\"] = datetime_np.month\n",
    "input_df[\"day\"] = datetime_np.day\n",
    "input_df = input_df.drop(labels=\"time\", axis=\"columns\")\n",
    "\n",
    "display(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e2400-02a4-472e-99e3-d3fdba0a1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for each column\n",
    "input_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7d1e7-6299-4d89-9218-ba5c7706d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format input data \n",
    "labels_np = input_df[\"precip_classes\"].values\n",
    "\n",
    "# Convert to numpy array\n",
    "features_list = [\"slp_anom\",\"hgt_detrended_anom\",\"year\",\"month\",\"day\"]\n",
    "features_np = input_df[features_list].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4a397-dbb8-4f2c-b90b-697800f846f3",
   "metadata": {},
   "source": [
    "Confirm that the time index matches for both dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b6f94-af2b-4e90-8fbb-7e5980a6afb6",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cec3b0-0d9b-4160-9fef-40305d9361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "# Tunable Parameter: Describes the proportion of the dataset we want to use for testing. 1 - split_size is used for training. \n",
    "split_size = 0.25\n",
    "\n",
    "# PARAMETERS:\n",
    "#     test_size: fraction of testing/validation datasets\n",
    "#     random_state: random parameter\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features_np, \n",
    "    labels_np, \n",
    "    test_size=split_size, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc3d0-e44c-416f-9025-dd0177838db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44280059-f6c7-4da0-8883-fec1088814e4",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28a2b1-ed86-4291-be05-eb4a60d415b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunable Parameters for Model\n",
    "number_of_trees = 10\n",
    "tree_depth = None \n",
    "node_split = 2       # minimum number of training samples needed to split a node\n",
    "leaf_samples = 1     # minimum number of training samples required to make a leaf node\n",
    "criterion = 'squared_error'    # variance reduction, alternatively 'mae'\n",
    "RAND_STATE = 24\n",
    "\n",
    "# Instantiate model with number of decision trees prescribed above\n",
    "# PARAMETERS:\n",
    "#     n_estimators: number of trees/ensembles\n",
    "#     random_state: random seed\n",
    "#     max_depth: maximum depth of each tree\n",
    "#     criterion: evaluation statistic to split a mode, 'mse'  or 'mae'\n",
    "#     min_samples_split: minimum number of samples needed to split a node\n",
    "#     min_samples_leaf: minimum number of samples needed to make a leaf\n",
    "#     for more, see: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = number_of_trees, \n",
    "                           random_state = RAND_STATE,\n",
    "                           min_samples_split = node_split,\n",
    "                           min_samples_leaf = leaf_samples,\n",
    "                           criterion = criterion,\n",
    "                           max_depth = tree_depth)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d8c7-aca3-4af5-abc0-1866d724f614",
   "metadata": {},
   "source": [
    "## Establish a baseline \n",
    "What to use here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c070e45-9352-4a1e-9fa4-10e36e872fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I NEED A BASELINE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22638940-47b3-4c53-bec0-4fd91d1fabda",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b69293-378a-4ebb-b0ec-2b8b8e72d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the forest's predict method on the test data\n",
    "# predictions = rf.predict(test_features)\n",
    "\n",
    "# # Use testing set to validate the performance\n",
    "# # Print out the mean absolute error (MAE)\n",
    "# mae_errors = abs(predictions - test_labels)\n",
    "# print('Baseline error (MAE): ', round(np.mean(mae_baseline_errors), 2))\n",
    "# print('Error (MAE): ', round(np.mean(mae_errors), 2))\n",
    "\n",
    "# # See its performance (mean squared errors)\n",
    "# mse_errors = np.sqrt(np.mean((predictions - test_labels)**2))\n",
    "# print('Baseline error (MSE): ', round( mse_baseline_errors, 2))\n",
    "# print('Error (MSE): ', round(mse_errors, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
