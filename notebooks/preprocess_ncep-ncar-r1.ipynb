{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab22f6f-3dc1-4f41-ae49-174bc29da320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-14T16:32:47.344655Z",
     "iopub.status.busy": "2023-09-14T16:32:47.343695Z",
     "iopub.status.idle": "2023-09-14T16:32:47.362130Z",
     "shell.execute_reply": "2023-09-14T16:32:47.358972Z",
     "shell.execute_reply.started": "2023-09-14T16:32:47.344592Z"
    }
   },
   "source": [
    "# Preprocesses NCEP-NCAR-R1\n",
    "Notebook preprocessing is based on the workflow in [read_reanalysis.ipynb](https://github.com/fdavenport/GRL2021/blob/main/notebooks/0a_read_reanalysis.ipynb) from Davenport and Diffenbaugh, 2021 \n",
    "<br><br>\n",
    "**Preprocessing steps**: \n",
    "1) Clip to Colorado\n",
    "2) Compute average over region (where should this step go?)\n",
    "3) [HGT only] Detrend the data\n",
    "4) Compute daily standardized anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67b1611-c90c-42ba-96e7-61bec01edd79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:03:51.362861Z",
     "iopub.status.busy": "2023-09-17T18:03:51.361327Z",
     "iopub.status.idle": "2023-09-17T18:03:53.803087Z",
     "shell.execute_reply": "2023-09-17T18:03:53.801742Z",
     "shell.execute_reply.started": "2023-09-17T18:03:51.362722Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sys \n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "# Import helper functions \n",
    "sys.path.insert(0, '../utils')\n",
    "from preprocessing_utils import (\n",
    "    get_state_geom, \n",
    "    convert_lon_360_to_180, \n",
    "    clip_to_geom, \n",
    "    calc_anomalies\n",
    ") \n",
    "from misc_utils import format_nbytes\n",
    "import parameters as param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf94d1-bb1b-4a9f-a6af-7a951a24bf3a",
   "metadata": {},
   "source": [
    "## Get Colorado state boundary geometry \n",
    "Will be used to clip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c0c529-63c0-4552-af83-452e3e0b2f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:03:53.808061Z",
     "iopub.status.busy": "2023-09-17T18:03:53.807256Z",
     "iopub.status.idle": "2023-09-17T18:03:55.721284Z",
     "shell.execute_reply": "2023-09-17T18:03:55.720098Z",
     "shell.execute_reply.started": "2023-09-17T18:03:53.808016Z"
    }
   },
   "outputs": [],
   "source": [
    "state = \"Colorado\"\n",
    "geom = get_state_geom(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c06b85-95b3-4789-aae7-a7fae5da0ff9",
   "metadata": {},
   "source": [
    "## Sea Level Pressure data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbecdcb-f6bb-437b-b9a3-c054f2e9626b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:03:55.723999Z",
     "iopub.status.busy": "2023-09-17T18:03:55.723525Z",
     "iopub.status.idle": "2023-09-17T18:04:01.194647Z",
     "shell.execute_reply": "2023-09-17T18:04:01.193232Z",
     "shell.execute_reply.started": "2023-09-17T18:03:55.723935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_identify: Cannot find proj.db\n",
      "ERROR 1: PROJ: proj_identify: Cannot find proj.db\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing with an out-of-order index is generating 20 times more chunks\n",
      "  return self.array[key]\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing with an out-of-order index is generating 20 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "# Open dataset \n",
    "var = \"slp\" # Variable name \n",
    "filepaths_wildcard = \"../data/{0}_daily_means/{1}*.nc\".format(var,var)\n",
    "filepaths_all = glob(filepaths_wildcard)\n",
    "ds = xr.open_mfdataset(filepaths_all).sel(time=param.time_period)\n",
    "global_attrs = ds.attrs\n",
    "ds = ds.drop_dims(\"nbnds\")\n",
    "\n",
    "# Convert lon range from 0:360 to -180:180 \n",
    "ds = convert_lon_360_to_180(ds)\n",
    "\n",
    "# Clip to Colorado geometry \n",
    "ds = clip_to_geom(ds, geom)\n",
    "\n",
    "# Average over entire region\n",
    "ds = ds.mean(dim=[\"lat\",\"lon\"])\n",
    "\n",
    "# Calculate daily standardized anomalies\n",
    "ds = calc_anomalies(ds, var) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f781e439-f4bd-4c28-ad90-29b2cf607f5a",
   "metadata": {},
   "source": [
    "Format the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d1d399-b87a-436e-a56c-2f3b750573c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:04:01.197502Z",
     "iopub.status.busy": "2023-09-17T18:04:01.196961Z",
     "iopub.status.idle": "2023-09-17T18:04:20.099908Z",
     "shell.execute_reply": "2023-09-17T18:04:20.098583Z",
     "shell.execute_reply.started": "2023-09-17T18:04:01.197433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of output dataset: 85.61 KB\n"
     ]
    }
   ],
   "source": [
    "# Format the output data \n",
    "output_da = ds[var+\"_anom\"]\n",
    "output_da.attrs = {\n",
    "    \"long_name\": \"mean daily sea level pressure anomalies\",\n",
    "    \"units\": \"Pa\",\n",
    "}\n",
    "output_ds = output_da.to_dataset()\n",
    "output_ds.attrs = global_attrs\n",
    "output_ds.attrs[\"region\"] = \"Data has been spatially averaged across the state of \"+state\n",
    "output_ds.attrs[\"title\"] = global_attrs[\"title\"] + \" modified to produce daily anomalies\"\n",
    "output_ds.attrs[\"history\"] = global_attrs[\"history\"] + \"\\nDaily anomalies produced \" + datetime.today().strftime('%Y/%m/%d')\n",
    "\n",
    "# Print size of dataset \n",
    "nbytes = format_nbytes(output_ds.nbytes)\n",
    "print(\"Size of output dataset: {0}\".format(nbytes))\n",
    "\n",
    "# Convert to pandas DataFrame \n",
    "output_df_slp = output_ds.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a8725-6c59-4476-ab2f-490580186bb7",
   "metadata": {},
   "source": [
    "# Geopotential Heights at 500 hPa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cb5577-c09c-49bd-b834-82cdfbf3fdad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:04:20.102511Z",
     "iopub.status.busy": "2023-09-17T18:04:20.102164Z",
     "iopub.status.idle": "2023-09-17T18:04:47.902113Z",
     "shell.execute_reply": "2023-09-17T18:04:47.900503Z",
     "shell.execute_reply.started": "2023-09-17T18:04:20.102488Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_identify: Cannot find proj.db\n",
      "ERROR 1: PROJ: proj_identify: Cannot find proj.db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope of trend: 0.675666884206026 m per year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing with an out-of-order index is generating 19 times more chunks\n",
      "  return self.array[key]\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing with an out-of-order index is generating 19 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "# Open dataset \n",
    "var = \"hgt\"\n",
    "filepaths_wildcard = \"../data/{0}_daily_means/{1}*.nc\".format(var,var)\n",
    "filepaths_all = glob(filepaths_wildcard)\n",
    "ds = xr.open_mfdataset(filepaths_all)\n",
    "global_attrs = ds.attrs\n",
    "\n",
    "# Clean it up a bit \n",
    "level = 500\n",
    "ds = ds.sel(time=param.time_period)\n",
    "ds = ds.drop_dims(\"nbnds\")\n",
    "ds = ds.sel(level=level).drop(\"level\") \n",
    "\n",
    "# Convert lon range from 0:360 to -180:180 \n",
    "ds = convert_lon_360_to_180(ds)\n",
    "\n",
    "# Clip to Colorado geometry \n",
    "ds = clip_to_geom(ds, geom)\n",
    "\n",
    "# Average over entire region\n",
    "ds = ds.mean(dim=[\"lat\",\"lon\"])\n",
    "\n",
    "# Calculate annual domain average 500-hPa GPH to remove seasonal variability \n",
    "domain_mean_df = ds[var].groupby('time.year').mean(dim = \"time\").to_dataframe(name = var)\n",
    "\n",
    "# Calculate linear trend in 500-hPa GPH\n",
    "trend = np.polyfit(domain_mean_df.index.get_level_values('year'), domain_mean_df[var], 1)\n",
    "print(\"Slope of trend:\", trend[0], \"m per year\")\n",
    "\n",
    "# Calculate detrended hgt\n",
    "ds['change'] = (ds.time.dt.year - int(param.time_start))*trend[0]\n",
    "ds[var+'_detrended'] = ds[var] - ds['change']\n",
    "ds = ds.drop_vars('change')\n",
    "\n",
    "# Calculate daily standardized anomalies\n",
    "ds = calc_anomalies(ds, var+'_detrended') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000edbe8-d2d2-4762-b81d-31028dbde1fe",
   "metadata": {},
   "source": [
    "Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c58c39-6484-40a9-a8e5-07d09aadeb75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:04:47.905332Z",
     "iopub.status.busy": "2023-09-17T18:04:47.904228Z",
     "iopub.status.idle": "2023-09-17T18:05:12.228049Z",
     "shell.execute_reply": "2023-09-17T18:05:12.226267Z",
     "shell.execute_reply.started": "2023-09-17T18:04:47.905282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of output dataset: 108.44 KB\n"
     ]
    }
   ],
   "source": [
    "# Format the output data \n",
    "output_da = ds[var+\"_detrended_anom\"]\n",
    "output_da.attrs = {\n",
    "    \"long_name\": \"mean detrended daily geopotential height anomalies\",\n",
    "    \"units\": \"m\",\n",
    "    \"level\":level\n",
    "}\n",
    "output_ds = output_da.to_dataset()\n",
    "output_ds.attrs = global_attrs\n",
    "output_ds.attrs[\"region\"] = \"Data has been spatially averaged across the state of \"+state\n",
    "output_ds.attrs[\"title\"] = global_attrs[\"title\"] + \" modified to produce detrended daily anomalies\"\n",
    "output_ds.attrs[\"history\"] = global_attrs[\"history\"] + \"\\nDaily detrended anomalies produced \" + datetime.today().strftime('%Y/%m/%d')\n",
    "\n",
    "# Print size of dataset \n",
    "nbytes = format_nbytes(output_ds.nbytes)\n",
    "print(\"Size of output dataset: {0}\".format(nbytes))\n",
    "\n",
    "# Convert to pandas DataFrame \n",
    "output_df_hgt = output_ds.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb626e-1082-4795-8eac-bd72aa701d6a",
   "metadata": {},
   "source": [
    "## Combine datasets and write to csv\n",
    "Combine DataFrames for both variables <br>\n",
    "Save to local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b1d4fb-1c0c-4e57-bb0a-6ec3104c9f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:05:12.231086Z",
     "iopub.status.busy": "2023-09-17T18:05:12.230650Z",
     "iopub.status.idle": "2023-09-17T18:05:12.279310Z",
     "shell.execute_reply": "2023-09-17T18:05:12.278096Z",
     "shell.execute_reply.started": "2023-09-17T18:05:12.231028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>slp_anom</th>\n",
       "      <th>hgt_detrended_anom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>0.364426</td>\n",
       "      <td>0.631857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>0.758293</td>\n",
       "      <td>0.485430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-03</td>\n",
       "      <td>1.545197</td>\n",
       "      <td>1.384811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-04</td>\n",
       "      <td>0.962102</td>\n",
       "      <td>1.977247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-05</td>\n",
       "      <td>0.158440</td>\n",
       "      <td>1.538378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  slp_anom  hgt_detrended_anom\n",
       "0 2001-01-01  0.364426            0.631857\n",
       "1 2001-01-02  0.758293            0.485430\n",
       "2 2001-01-03  1.545197            1.384811\n",
       "3 2001-01-04  0.962102            1.977247\n",
       "4 2001-01-05  0.158440            1.538378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df_final = output_df_slp.merge(output_df_hgt, on=\"time\").reset_index()\n",
    "output_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db218f8-aed2-4602-9398-7a079a854dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:05:12.281329Z",
     "iopub.status.busy": "2023-09-17T18:05:12.280724Z",
     "iopub.status.idle": "2023-09-17T18:05:12.372979Z",
     "shell.execute_reply": "2023-09-17T18:05:12.371877Z",
     "shell.execute_reply.started": "2023-09-17T18:05:12.281287Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"slp_hgt_anoms.csv\"\n",
    "output_df_final.to_csv(\"../data/input_data_preprocessed/{}\".format(filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0338c-a7d9-4b6e-b428-68054d2dea94",
   "metadata": {},
   "source": [
    "## Output xarray object to zarr store in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16eb0656-f1fa-46b4-a7d1-d77d7d046527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:05:12.374737Z",
     "iopub.status.busy": "2023-09-17T18:05:12.374463Z",
     "iopub.status.idle": "2023-09-17T18:05:12.384984Z",
     "shell.execute_reply": "2023-09-17T18:05:12.383327Z",
     "shell.execute_reply.started": "2023-09-17T18:05:12.374715Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Confirm that you're connected to the right S3 bucket\n",
    "# s3 = boto3.resource(service_name='s3')\n",
    "# for bucket in s3.buckets.all():\n",
    "#     # What is printed here should match the variable \"bucket\" below\n",
    "#     print(\"Bucket in S3: \" + bucket.name)\n",
    "\n",
    "# # S3 paths and such \n",
    "# bucket = \"ml-extreme-precip\" # Name of bucket \n",
    "# folder = \"NCEP-NCAR-R1\" # Name of folder in bucket\n",
    "# s3_path = \"s3://{0}/{1}/\".format(bucket, folder) \n",
    "\n",
    "# # Name to give file \n",
    "# # DO NOT include file extension (this will be .zarr)\n",
    "# filename = output_da.name\n",
    "\n",
    "# # Path to zarr store in AWS bucket\n",
    "# filepath_zarr = \"{}{}.zarr/\".format(s3_path, filename)\n",
    "# print(\"zarr store will be written to path: {}\".format(filepath_zarr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a14729-7797-4ec6-ae36-e1f5ec6088ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:05:12.387579Z",
     "iopub.status.busy": "2023-09-17T18:05:12.387102Z",
     "iopub.status.idle": "2023-09-17T18:05:12.409357Z",
     "shell.execute_reply": "2023-09-17T18:05:12.408311Z",
     "shell.execute_reply.started": "2023-09-17T18:05:12.387543Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Write zarr to bucket \n",
    "\n",
    "# # Initilize the S3 file system\n",
    "# s3 = s3fs.S3FileSystem()\n",
    "# store = s3fs.S3Map(root=filepath_zarr, s3=s3, check=False)\n",
    "\n",
    "# # Save to zarr\n",
    "# output_ds.to_zarr(\n",
    "#     store=store, \n",
    "#     consolidated=True, \n",
    "#     mode=\"w\" # Overwrite any existing files \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7353fc83-325e-4245-bd97-0e51d458cd7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-17T18:05:12.411432Z",
     "iopub.status.busy": "2023-09-17T18:05:12.410679Z",
     "iopub.status.idle": "2023-09-17T18:05:12.439449Z",
     "shell.execute_reply": "2023-09-17T18:05:12.438362Z",
     "shell.execute_reply.started": "2023-09-17T18:05:12.411390Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Now try opening the file from AWS! :D \n",
    "# xr.open_zarr(filepath_zarr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
